Previous from [[L4 - 03-03-2023]]
# Key look-up - Kademlia DHT

Given an identifier, retrieve the associate content/node using the XOR metric for minimum distance (*closest node to the key*).
![[Pasted image 20230308112052.png]]
In the previous figure, is considered $k = 1$ but usually $k \ge 1$. 
In step 2, the subject node is the **blue node** that contains the reference to the green node, contained in its routing table. 
![[Pasted image 20230308112246.png]]

The procedure is *iterative* because the routing information are returned to the original querying node: node $n$ sending the lookup request manages all the search process and at eacj routing step, the node waits for a reply that includes a notification of the next routing step. 

To efficiently find the target node, a **parallel routing** is necessary to discover the target node even by routing the requested to the most distant node in the current node routing table. 
An example is reported here:
![[Pasted image 20230308112720.png]]
The *blue node* $0011$ looks for the red node $1101$: it has a reference to the green nodes ($1001$ and $1110$) with: 
			$dist (1101, 1001)=4$, and $dist(1101,1110)= 3$
So it's possible that thenode 1001 which is more distant from the target has a reference to the target while the closest node 1110 has no reference to it. The paralle routing allows the blue node to **send requests to both nodes**. 
The degree of parallelism is defined by $\alpha$. 
The query is sent to $\alpha \ge 1$ nodes closest to target: *the unidirectionality of the XOR metric guarantees that all the pahts converge towards the target*. 

## Basic Protocol Operations
Kademlia cnsists of 4 primitives (*not iterative*) operations, defined as *RPCs* that exploits UDP. 
1. `FIND_NODE(v) -> w (T)`: `v,w` are nodes while `T` target if the lookup. The recipient of the message `w` return `k` (`IP Add, UDP port, Node ID`) triples for the k nodes it knowns abut the closest to the target `T`. These triples can come from a single k-bucket, or they may come from multiple k-buckets if the closest k-bucket is not full.
2. `FIND_VALUE(v) -> w (T)`: the in parameter is a 160 bit ID representing a value. The returned value corresponds to T if is presented in the queried node (`w`) and the associated data is returned. If not present, is equivalent to `FIND_NODE` and  `w` return a set of `k` triples. So this operation return a list of other peers, it's up to the requester to continue searching fo the desired value from that list.
3. `PING`: probe node `w` to see if it's online
4. `STORE(v) -> w`: instruct node `w` to store a `<key,value>` pair.

## Node lookup
To be able to locate the `k` **closest node to a given ID** an iterative algorithm is followed based on the basic primitive `FIND_NODE`: many `FIND_NODE` operations are execute in paralle according to the pamater $\alpha$ that is the sysyem-wide **concurrency parameter**. Based on $\alpha$ value:
- *$\alpha = 1$*: lookup algorithm is similar to Chord, one step progress each ti,e
- $\alpha \ge 1$: allows the flexibility of choosing any one of $k$ nodes to forward a request to, at each test
The *lookup* of a node is exploited both for **finding nodes** and **finding values**: both this type of searching need to stop when the value/node is reached/founded.

#### Complete lookup example 
The node `P` looks for the key `Q` (*which is the identifier of a node/content*) so `P`:
1. look in the bucket list of the nodes closest to `Q`
2. looks in the $k$-bucket closest to the key and not empty: if the bucket includes less than $\alpha$ nodes, looks in close buckets.
3. selected contacts may belong to different buckets
![[Pasted image 20230310114149.png]]
This allows `P` to selects $\alpha$ nodes from the selected bucket: this allows `P` to sends the query in **parallel** to all the selected nodes (*using RPC `FIND_NODE(Q)`*). 
Each contacted node finds out, in turn, `k` nodes closer to the key: each node may exploit a different bucket of its routing table. 
The **routing is iterative** because:
1. Each node returns the results to `P`
2. The results are inseted in a list which is ordered on the basis of the distance between the node and `Q`
3. Node `P` continues the routing process through the results obtained from `P`

![[Pasted image 20230310114507.png]]
At each step, the node `P` selects $\alpha$ nodes from the received information: if it obtains nodes closer to the rarget with respect to the preceding nodes, it performs lookup on these nodes. Otherwise, chooses further nodes from those which have not been still contacted. 
The algorithm terminate when a round of `FINDW_NODE(T)` fails to return any closer node.
The entire algorithm is sketched here:
```bash 
k-closest= alpha contacts from the non-empty k-bucket closest to the key 

if there are fewer than a contacts in that bucket then 
		k-closest = k-closest U closest contacts from other buckets. 

closestNode = the closest node in k-closest 

/* recursive step 
repeat {  
	1. select from k-closest, alpha closest contacts which have not been queried yet 
	2. send in parallel, asynchronously FIND_NODE to the delected contacts each contact, if live, returns k nodes 
	3. add to k-closest the new received nodes and update closestNode 
	}
	
	until no node closer to the target than closestNode is returned 
	send in parallel asynchronously FIND_NODE to the k closest nodes it has not already queried 
	
	return the k closest nodes
```

To store a **(key, value)** pair, the lookup operation first find the k closest nodes to the key and sends them `STORE` RPCs. The contacts data is replicated on those nodes.

The **republishing mechanism** allows each node to re-publish the (key,value) pair to keep them alive. This mechanism is needed when some of the k nodes that initially get the pair leave the network or when new nodes enter the network with an identifier closer to key than the nodes on which the key-value paiir was originally published.
In Kademlia, the republish happen every 24 hours. 

### Node Join/Leave
The `new` node (*or joining node*) borrow an alive node's ID offline (to the *boostrap node `boot`*): the initial routing table of the joined new node has a single k-bucket containing `new` and `boot`. To enalrge the knowledge of the network, the node need to advertise himself and expand the entries of the routing table: to do this, the node send a `FIND_NODE` with the identifier ID of the node itself. By sending this message, other messages return the reference to the joined neighborhood nodes and other nodes insert the `new` node in their routing table. 
Some buckets corresponding to a range of identifiers can be empty: to enlarge the knowledge of those peers the node can also execute a `FIND_NODE` operation with as identifier a subset of the range missing in the routing table. 
When the routing table is full in each of $k$ buckets, we can also make an optimization by considering alternative paths by considering the TTL to contact a node: a node can misure the TTL with another one by using the `PING` primitives.

The **leaving operation** is minimal: the node leaving does not require further operations. If a node does not reply, it will be discarted from the k-buckets so by *auto-healing* the topology structucture created by the nodes disconnection. 


## Strenghts/Weaknesses
Between the weaknesses there is a 
1. non-uniform distribution of nodes in ID-space that also result into imbalanced routing table and inefficient routing. 
2. the balacing of storage load is not truly solved despite the uses of different path to get the target node: some techniques used in IPFS to solve the storage load balancing use a modification of distribute the content to other peers to be able to manage the load.

Despite, between Kademlia stenghts there are:
- low control message overhead 
- - tolerance to node failure and leave 
- capable of selecting low-latency path for query routing 
- provable performance bounds

See a summary here: https://kelseyc18.github.io/kademlia_vis/basics/1/

### Kamdelia as Prefix Matching DHT
Kamdelia is an instance of more general concept of prefix matching due to its routing mechanism based on XOR metrics on prefixes. 
The basic ideas is a generalization of the routing to hypercurbes: mapping of nodes and keys to the numbers of *`m`* digits of a certain base `base`. and assign each key to the node with which it shared the **longest prefix** (*if possible*). Different bases allows to change the pedix of the logarithmic factor in operations. It's the same idea as Kademlia but enlarged to a identification space of base and not only binary. 
Here the basic idea:
![[Pasted image 20230310120027.png | 400]]
The initial file request are served by a centralized server: further requests served by peers which have already received an replicated the files.


