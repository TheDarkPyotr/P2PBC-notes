# Introduction to Distributed Hash Tables

The problem of retrieve content in P2P network can be summarized by asking where the information should be stoed and how to find it without a centralized server. 
In this scenario, the action to carried out are two:
- *Searching*: search guided by the value of a set of attributes of the content
- **Addressing**: pair **unique identifier ID** to the content and use ID to retrieve it. Usually the **id** is the *hash of the content* obtained from applying a cryptographic function (*e.g. SHA*) that return a **fixed-length identifier**. 
	Surely the pros are the efficient object detection, the cons are the complexities of mantaining the addressing structure. 
In this scenario, those two action allows to drive towards a distributed network content like *IPFS*. 

### DHT motivations
Comparing two different solution, we have:
- *Centralized approach*: a server that index the data. The search time will be $O(1)$ and the spare required is $O(N)$ with `N =  Amount of shared content` and also complex query are easily managed.
- *Fully Distributed Approach*: in an unstructured network, the *search worst-case* is $O(N^{2})$ because each node contact each of its neighbours. Despite this, optimizations can be done by using *TTL or identifiers to avoid cyclic paths*, obtaining a serach cost of $O(N)$.
   Also the space required is $O(1)$ because does not depend on the number of nodes in the system so no data structure to route queries (*by flooding*). 

![[Pasted image 20230301120442.png]]
The key idea behing DHT is to use an *hash table* that have a $O(log(n))$ number of entries and allows to retrieve data in $O(log(n))$, avoid false negative and it's suitable for self-organizing systems with an high dynamic of node that leave/join. 


![[Pasted image 20230301120652.png]]

### Memcached
Memcached is a distributed memory-object caching system for dynamic web caching: a pool of caching server to provide fast access to information. This allows to reduce database server load so acces to the database only on cache misses. It's implemented by splitting an hash table in serveral parts hosted by differents server so to bypass memory limitation of a single computer. 

## Scaling out - Distribute Hashing
The key idea is split the hash table into several parts and sitributed it to several servers: use *hash of resources (or URLs of resources)* allows to map them to a dynamically changing set of web caches. So the URL's **hash** is the key to access the content and the chosen cryptographic function allow to map the key to a single server. Each machine (user) can locally compute which cache should contain the required resource, referenced by an URL (*this avoid inter-cache communication*). 

The **rehashing problem** describe how manage the system in case of distributed hash table: in case failure of one server the content of this server is lost so it's not suitable for large data chunk that need to be moved or *remapped* each failure. Let's make an example: suppose to have as cryptographic hash function a classical hash function that support 4 caching nodes. Store the resource with URL `x` at the cache, implies calculate:
			`SHA(x)  -> ID-160 bit % 4 ---> Cache ID`
Now, if the storage gets biffer so suppose to add more 2 caches: with 6 caches, need to recalculate where all the URLs are stoed. The only URL stored on the same node as before are those were:
			`SHA(URL) mod 4 == SHA(URL) mod 6`
so with 10 buckets, and 1000 keys abot 99% percent of keys have to be remapped and this implies an huge data traffic load on nodes. 

## Consistent hashing
It's an hash techniques that guarantees that adding/remove nodes allows to remove/move a small percentange of full data. 
The idea is to **map a contigous internval of hash values** to the same node, not a set of sparse value. To map an interval to a node, in addition to hashing the name of the object (*whenever they're URLs or IP addresses*), it's necessary to also hash the names of the nodes in same space (*same space means to the same co-domain; the identifier of a node is obtained by hashing their IP addresses*). 
The distribution scheme does not depend directly on the number of servers: each node manages an **interval of consecutive hash keys** and not a set of sparse keys. Intervals are **joined/splitted** when nodes join/leave the network and key ridistributed between adjacent peers. 



### Chord: construct a DHT
![[Pasted image 20230303115917.png]]

Chord is a [[DHT]] which uses a  **logical ring** as the data structure to distribute the contents.
Use a logical name space, called **identifier name space** consistiting of identifiers ${0,1,2,,....n}$ : they are defined as a logical ring *modulo* `n`. 
Eveyr node picks a random identifier through *hash H*, used both for nodes and content. 

The following image shows how nodes are mapped in the ring:![[Pasted image 20230303114227.png]]

 Each node also has a **successor** to it: the successor is defined via function `succ(NUMBER)`.  The parameter `NUMBER` belongs to the hashes set. The successor is the next hash in **clockwise direction.**   
The actual content is mapped to the ring. But in case the hash does not correspond to a node, it is mapped to the node **subsequent** to it. As can be seen in the latter figure:
![[Pasted image 20230303114328.png]]

As one node is **removed from the network, its content is remapped to its successor**:![[Pasted image 20230303114352.png]]
Instead of remapping all the keys, only the key of a leaving node are remapped. Having $n$ number of servers and $k$ number of keys, the ones to be remapped are $k/n$ on average.

## Node Join
As for the **introduction of the new nodes**, inserting a new node brings the *issue of remapping some of the content hashed between its successor and predecessor*. The following figures show the crown moving from 11 to 10, as 10 joins the network. As the successor of 9 now becomes 10 instead of 11.
![[Pasted image 20230303114516.png]]

There is the distinction of two types of leaves from the network: 
-   **Voluntary leave**: in this case all the content stored on the node is redistributed. All the other nodes remove it from their routing table.  
-   **Node failure**: to prevent any content loss that can happen in this case, there is the use of **data replication** to add redundancy to the ring. The routing tables are updated periodically by exploring the routing paths and finding offline nodes.
In a **voluntary leave**, for a node there is the partitioning of its address space to the neighbouring nodes. 

## Lookup  
Lookup for nodes only connected to one node (*who is its*) successor in the **routing table** is seen in the following code.  
The lookup is recursive, the nodes which the query is forwarded do not return the next node to lookup up. Instead they will recursively search for the originator. The result is returned at the end of the process to the originator. The originator does not control the query flow. 
To find the `successor` of an `ID` the `predocessor` must be founded first. This means entails travelling the logical circle. The `predocessor` is the first **non-NULL node in the ring** which as an ID associated.
The algorithsm to find the successor of a node, given the ID is:
![[Pasted image 20230303114856.png]]
Instead of connecting only to the successor, a node could store connection with other nodes in the interval of $(n+1, n+2, n+4, ..., n+2^{M-1})$ where:
- $M$ is the routing table size (*node table*)
- $N$ is the modulo
- $N$ ans $M$ are in a relation of $N = 2^{M} \leq M = log(N)$ which is the number of the entries in the routing table
So, in the following picture, starting from node 0 we can compute `succ(n+1), succ(n+2), succ(n+4), succ(n+8) ... `$succ(n+2^{M-1})$ . So for all $i$ in range $1..M$ every node known $successor(n+2^{i-1})$. 
![[Pasted image 20230303115451.png]]

The particularity of the system is how it searches for the successors if the current does not store the content. Also the content could end up in the from the previous node to the current node checked.
The nodes in the routing table of a node could be all the nodes (*but not likely, as it requires space*) or just the successor node: the latter case has a complexity of $O(N)$ to find a content, having  $N$ nodes. Its use can be seen in thefollowing snippet where instead of only asking for  the `successor` of a node, we also search for the `closestPrecedingNode` to *forward the query around the circle*. 

![[Pasted image 20230303121015.png]]
`Finger(i)` in the previous snippet is the **hash** of the node identified by $i$. The value computed is used to find the closest preceding node to the current node. Given that this is a blind area for the current node: this allows to find a node when the predocessor considered at first is NIL.
#### An example
![[Pasted image 20230303115814.png]]
This image shows the nodes in the routing table of a node and the lookup of an item on node 15.
The table has size  $N=16$, which means than $M=4$. 
Node 1 has $ID_{1}=1$ and the routing tables entries as: *2(ID1+2^0),3(ID1+2^1),5(ID1+ 2^2),9(ID1+2^3).*