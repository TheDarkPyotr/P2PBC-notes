# Kademlia DHT

Summarizing the Chord DHT, we have a virtual space address: we assign the keys to the first node encountered going clockwise starting from key. The keys are represented with the same colour of the node they are mapped to. This colors forms a partitions, as showed here:
![[Pasted image 20230303141647.png]]

**Kademlia** uses a different approach. It represent de facto standard searching alg. for P2P networks on the internet.
It's a protocol spefification for storing and retrieving data across P2P network charaterized by:
- *Decentralized*:
- *Fault Tolerant:*
- *Easy storing technique*: data is stored in key-value pairs

Respect to other DHTs, Kademlia have success because in query routing, the propagation of the query allows the peers to *enrich* their routing query. So propagating the query allows to add information on a peers table in a symmetric way. Also, the querying method allows to send the query in **parallel** trhough different paths. 
Kademlia also uses **iterative routing** so the peers that submitted the query is able to control the query ad each steps (*differently from recursive routing*). 

#### Structure of identifier space
The identifiers of node and data are organized in a **complete binary trie**: it comprises a *k-ary search tree or a prefix tree, allowing locate specific keys within a set*.
In the *trie*, edge are annotated with binary digits: a *path* identifies a key whose value may be stored in the leaf. 
![[Pasted image 20230303142817.png]]

The logical identifier space is defines by the leaves of the tree, so nodes and content have identifiers in the leaves. In the image, the circled binary corresponds to the nodes. Nodes are positioned as leaves in the trie (*sure not all leaves corresponds to peers*). It's necessary to define a rule to partition the keys (*contnet*) among the nodes: this should also respect the rules of consistent hashing. 
A new node entering the network now is part of the trie but each node does not store the entire trie on its own but only a subset of them that allows to guarantee a logarithmic cost of operations.

##### Mapping keys to nodes
A key is assigned to the node with the *lower common ancestor*: first find the **longest prefix** between the key and the node identifier and assign the key to that node.
![[Pasted image 20230303143423.png]]
In the figure, leaves are pointed by the vertical arrows and corresponds the nodes. The pale colored leaves are assigned to the circled nodes. In the example, the red leaves are in common `0` so the assigned peer is `000`, in blue despite `100` and `101` shared `10`, they are assigned to peer `110`: this is an arbitrary choice becayse they only shared with the two peers in the same subtree only `1`, in this case the choice was `110` but also can be `111`. 
		This *arbitrary property* can be exploit to alleviate a known problem in DHT about balancing the workload among peers: despite the using of cryptographic hash function, in a scenario with popular content highly requested and other content never requested, this can create a worklod balancing problem. 

If two computers tie for the longest common prefix, they must look to the *Most signicant bit (MSB)* where they differ, of index $b$ so assign the key to the node whose id bit $b$ equals bit $b$ of the key.
As an example, consider key 100 and nodes ids 110 and 111. The keys and the node ids share a common prefix of 1. The node ids 110 and 111 differ in the last bit so the last bit of the key is 0: assign the key to 110.

To compute the **closeness (or distance)** between a key and a node:
1. Initialize the distance value at 0
2. Compare the key and compute ID bit by bit
3. If the key and the node id differ at the $i^{th}$ least significant bit, add a penalty of $2^{i}$ to te distance value. 
4
It corresponds to the ***XOR Operation***: it compute the distance betwwen the identifiers and the nodes, interpreted as an *unsigned integer*. 
![[Pasted image 20230303144637.png]]

![[Pasted image 20230303144852.png]]
The clostet node is the **Node B** because  $4 < C$ and $4 < 9$ and should store **ID, 94.29.160.5,  34665**.
![[Pasted image 20230304104122.png]]
The XOR operation represent a valid **metric for the distance** and guarantees properties like:
- $d(x,x) = 0$
- $d(x,y) > 0,$ if $x!=y$
- $\forall x,y: d(x,y) = d(y,x)$ - ***Symmetry***
- $d(x,y) \oplus d(y,z) = d(x,z)$ - ***Transitivity***
- $d(x,y) \oplus d(y,z) \geq d(x,z)$ - ***Triangular inequality***
- ***Unidirectionality***: there is a single node at minimal distance with the key. 
  Given $x$ and a distance $\delta$ , it exists a single $y$ such that $d(x,y) = \delta$. For example, if $x=1001, \delta=0001$ the only point at distance $\delta$ from $x$ is $y = 1000$. 


The symmetry is showed in a geometric interpretation: consider 4 peers (*colored one*) the y-axis is the **distance between the colored peers and the peer on the x-axis**. 
![[Pasted image 20230304104327.png]]
The distance graph looks the same in both halves but *shifted* along the y-axis: *smaller the distance with any per in the same half space*.
The **symmetric** property allows Kademlia to learn contacts from ordinary queries it receives, helping building the *routing tables* (*not symmetric distances like in chords does not allow this*) while **undirectional** property allows to determine a single node ad a *minimal distance* with the key so lookup for the same key **converge to the same path** so caching items along this path is good to avoid hotspots. 


Taking two nodes identifiers, **the larger the common prefix, the smallest is the distance (*computed by XOR*)** so close nodes are charetrized by a long common prefix. Here an example:
![[Pasted image 20230303145655.png]]
![[Pasted image 20230303145932.png]]
Two leaves may be close in the tree and also numerically close, but they are **distant** according to the metrics given by the XOR:

$1000 \oplus 0111 = 1111 = 15$ (*which is also the maximum distance in this scenario*) while the *numerical difference* between is 1. 

#### Distances and identifier tree
Consider two identifiers $x$ and $y$ of length $L$ that share a **common prefix** of lenght $p$ and differ in the last $i = L - p$ bits, their distance, according to XOR metric will be s.t.:
									$2^{i-1} \leq d(x,y) \le 2^{i}$
![[Pasted image 20230304104814.png]]
This enables to **pair the nodes** of the subtree with an **identifier range**.
Consider this scenario:
![[Pasted image 20230304104911.png]]

The leaf $1000$ and $0111$ have a shared prefix length of $0$: the distance varies according to 
																$2^{3} \leq d \le 2^{4}$
![[Pasted image 20230304105059.png]]
 ![[Pasted image 20230304105143.png]]
 In the last pictures, the distance between the two nodes in the figure is *minimal*: they differ only in the last bit so $2^{0} \leq d \le 2^{1}$  and $0110 \oplus 0111 = 0001 = 1$.

## Peers tree: an alternative representation
![[Pasted image 20230304105337.png]]
The peers in the network are much lesser than the identifiers because the space of identifier is huge so not all the identifiers are paired with a peer.
The *node/peer tree* is an **unbalanced binary tree** showing only the identifiers of peers present in the network (*subset of all identifiers*): **each leaf of the tree is a peer, not for all identifiers**. So a leaf in the node tree corresponds to an identifier prefix: the peer paired with the leaf is the unique node with that prefix so there is a unique correspondence between peers and identifiers (*e.g $0011$ uniquely identifies the red peer, no other peer have the same prefix and the deepest part of the path is not useful to identify the peer.*)



### Routing table
The main goal is to define look-up operation and store only addresses of a small amount of nodes, guarantee a runtime of $O(log(n))$ by storing  a log number of node ids and their corresponding IP addresses along some contact taken from the identifier trie. 
![[Pasted image 20230304105841.png | 600 ]]

For a given node, take the the path from that node to the root: for every **sibling node** of a node met on that path 
1) store the identifiers of *k* nodes whose ids are descendent of the sbling
2) create a *sibling bucket* that contains the *k* contact for each level, forming a  **k-bucket**.

The image shows the contacts in the routing table of the red peer, including  $k = 2$ contact for each subtree, forming 4 different buckets (*in yellow the buckets that contains the contact in the routing table of the **red peer**)*. 
![[Pasted image 20230303151057.png]]

The rows of the routing table are **k-buckets*** ($1 \leq i \leq 160$): each row contans $k$ contact and corresponds to a subtree. The contact are stored a `(ID, IP, UDP Port)` and each row contains the contact with distance $d$ from the peer name where $2^{i-1} \leq d \le 2^{i}$. So each entry corresponds to a *common prefix*: the lower the entry the longest the common prefix. 
![[Pasted image 20230304110327.png]]
Each k-bucket corresponds to a prefix and covers a subset of the identifier space: the set of all the k-buckets cover the whole identifier space. The first entries of the routing table correspond to peers sharing a *long prefix* with the owner of the routing table.
The **last entry** of the routing table corresponds to peers sharing a smaller prefix and cover a larget set of identifiers: may include a larger number of contacts but never more than $k$ contacts.
The value of $k$ is defined such that the **probability that a crash of more of $k$ nodes is a rare event**. 
The nodes in each bucket are mantained in ordered such that **least recently contacted nodes are in the first positions of the list**. 

### K-Buckets management: add contact
![[Pasted image 20230303151751.png]]
As shown in the flow, the last peer contacted is putted at the end of the table. If the information does not exists in the buckets, check if the bucket is full: if not remove the least recently node (first position of the list) and if there is no response drop it, applying a sort of *self-healing* update.
This approach prefers to mantain the oldest nodes as first choice instead of the new one: this choice is based on analysis made on Gnutella protocol tracing data that shown that *the longer a node has been up, the more likley it's to remain up another hour* so by keeping the oldest live contacts around, k-buckets **maximize** the porbability that the nodes they contain will remain online. This mechanism is also known as **Least Recently Seen Eviction**.
Other secondary benefit of the k-buckets is the resistance to the DoS attacks where an attacker cannot flush the nodes routing state by flooding the system with new nodes. 

The k-buckets are **refreshed** for each query passing through the node: if a node has left the network, new information received from the queries refreshes the k-bucket list. 
Can also happen thath a k-bucket is not refeshed for a given period of time due to the lack of messages from nodes in the range covered by the k-bucket. 
Usually a refresh is **periodically executed by the protocol**: Kademlia chooses an identifier belonging to the range covered by the bucket at random and search the identifier so if the node with that identifier sends a reply it is inserted in the k-bucket.


## Key look-up
The query is *something* i'm looking for a given ID: the lookup operation find the closest node to the key in your routing table. 




